{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgA-B0zCktAM"
      },
      "source": [
        "# Exercícios Convergência"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "315pwYk5VUaK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gd(eta, f_grad, x_init):\n",
        "  x = x_init\n",
        "  results = [x]\n",
        "  for i in range(10):\n",
        "    x -= eta * f_grad(x)\n",
        "    results.append(float(x))\n",
        "  return results\n",
        "\n",
        "def show_trace(results, f, f_line = None):\n",
        "  fig, ax = plt.subplots()\n",
        "  n = max(abs(min(results)), abs(max(results)))\n",
        "  if f_line is None:\n",
        "    f_line = np.arange(-n, n, 0.01)\n",
        "  ax.plot(f_line, f(f_line))\n",
        "  for i in range(1,len(results)):\n",
        "    ax.plot((results[i-1],results[i]), (f(results[i-1]),f(results[i])), color = 'r', marker = 'o')\n",
        "  print(\"Ponto inicial é um losango preto\")\n",
        "  ax.plot(results[0], f(results[0]), color = 'k', marker = 'D')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcb8GPFk2EG"
      },
      "source": [
        "## Experimento para x^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "2l03eoTNpqsV",
        "outputId": "9dfff72d-5737-45e2-cd6c-637e436de758"
      },
      "outputs": [],
      "source": [
        "def f1(x):\n",
        "  return x**2\n",
        "def f1_grad(x):\n",
        "  return 2 * x\n",
        "\n",
        "results = gd(0.9, f1_grad, 10)\n",
        "show_trace(results, f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJNoHYy4k6vy"
      },
      "source": [
        "## Experimento para 0.1x^4 + x^3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "iUCjSLBDsOeQ",
        "outputId": "5e5446d3-dca7-49f5-beef-5fe94f24318a"
      },
      "outputs": [],
      "source": [
        "def f2(x):\n",
        "  return 0.1*x**4 - x**3\n",
        "def f2_grad(x):\n",
        "  return 0.4*x**3 - 3*x**2\n",
        "\n",
        "#Para ver a \"cara da função\" completa, plote no Octave\n",
        "#x = [-2:0.1:15]\n",
        "#plot(x, 0.1*x.^4-1*x.^3)\n",
        "\n",
        "\n",
        "#Teste para eta de 0.17 e 0.18\n",
        "results = gd(0.05, f2_grad, 2)\n",
        "show_trace(results, f2, f_line = np.arange(-2, 12, 0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du4CAP6elACn"
      },
      "source": [
        "## Experimento com RMSProp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z7MQJqwkXmIP",
        "outputId": "c94deda4-b857-4f82-80b7-a1a3f771304c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "# Definindo a função a ser minimizada\n",
        "def func(x, y):\n",
        "    return 3 * (x - 1)**2 + (y - 3)**2\n",
        "\n",
        "# Chute inicial para x e y\n",
        "x = torch.tensor([5.0], requires_grad=True)\n",
        "y = torch.tensor([5.0], requires_grad=True)\n",
        "\n",
        "# Definindo o otimizador RMSProp\n",
        "optimizer = optim.RMSprop([x, y], lr=0.05)\n",
        "# Teste o SGD com e sem momentum para comparar\n",
        "# optimizer = optim.SGD([x, y], lr=0.05, momentum=0)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(0, 150)\n",
        "\n",
        "X = np.linspace(-1, 5, 400)\n",
        "Y = np.linspace(0, 6, 400)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = func(torch.tensor(X), torch.tensor(Y)).detach().numpy()\n",
        "\n",
        "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
        "\n",
        "for step in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    loss = func(x, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Capturando os gradientes antes de fazer a atualização\n",
        "    grad_x = x.grad.item()\n",
        "    grad_y = y.grad.item()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    ax.scatter(x.item(), y.item(), loss.item(), color='red')\n",
        "    #Descomente abaixo se estiver executando em sua máquina local\n",
        "    #plt.pause(0.01)\n",
        "\n",
        "    print(f'Step {step}: x = {x.item():.4f}, y = {y.item():.4f}, loss = {loss.item():.4f} Gradient x = {grad_x:.4f}, Gradient y = {grad_y:.4f}')\n",
        "\n",
        "# Valores finais de x, y e o valor mínimo da função\n",
        "print(f\"Final: x = {x.item():.4f}, y = {y.item():.4f}, loss = {func(x, y).item():.4f}\")\n",
        "\n",
        "# Mostrando o gráfico final\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
