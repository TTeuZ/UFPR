{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["5HvbucbofsGI","peBJO6qMgSR4"],"mount_file_id":"10GqPe54h3vLuYzOuNCKtBHBvZ4A8zReL","authorship_tag":"ABX9TyOCfnOyVU49yhFpMJIcshGX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kjzztfGwe1XX","executionInfo":{"status":"ok","timestamp":1696719053592,"user_tz":180,"elapsed":1640,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"outputs":[],"source":["import math\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","from numpy import sin, cos, arccos, pi, round\n","\n","from sklearn import neighbors\n","from sklearn import linear_model\n","from sklearn import svm\n","from sklearn import tree\n","from sklearn.metrics import mean_squared_error, mean_absolute_error"]},{"cell_type":"markdown","source":["### Leitura e tratamento inicial dos dados\n","\n","Lendo a CSV e removendo todos os dados com valores de Tp_est iguais a zero"],"metadata":{"id":"Dp3ndF8DfZRl"}},{"cell_type":"code","source":["df_raw_data = pd.read_csv('/content/drive/MyDrive/BCC - UFPR/Semestres /9 - 2023-2/Aprendizado de Maquina/Lab2/Dados_Radar_Estacao_Completo_2018_2022.csv')\n","df_raw_data.drop(df_raw_data[df_raw_data['Tp_est'] == 0.0].index, inplace=True);"],"metadata":{"id":"apl-nNRqfdxS","executionInfo":{"status":"ok","timestamp":1696721968499,"user_tz":180,"elapsed":14462,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["Remoção inicial de colunas da base geral (2018 a 2022). A colunas removidas foram:\n","- Unnamed: 0 pois é a coluna de ids\n","- latitude e longitude pois possuem a mesma informação que as colunas lat e lon;\n","- distancia: Não agrega valor ao modelo. (*)"],"metadata":{"id":"ZfVDhdkUffjS"}},{"cell_type":"code","source":["df_raw_data.drop(['Unnamed: 0', 'latitude', 'longitude', 'distancia'], axis=1, inplace=True)"],"metadata":{"id":"VDVwGtoxXAJ-","executionInfo":{"status":"ok","timestamp":1696721968499,"user_tz":180,"elapsed":21,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Separando o raw data entre treinamento e teste. Anos de 2018 a 2021 para treinamento e 2022 para teste.\n"],"metadata":{"id":"niHUe8qRfkJ1"}},{"cell_type":"code","source":["raw_train_test_group = df_raw_data.groupby(df_raw_data['time'].str.contains('2022'))\n","\n","df_raw_train = raw_train_test_group.get_group(False).copy()\n","df_raw_test = raw_train_test_group.get_group(True).copy()"],"metadata":{"id":"bpE1520yfk37","executionInfo":{"status":"ok","timestamp":1696721968499,"user_tz":180,"elapsed":19,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["Separando os dados de treinamento, selecionando uma porção dos dados para validação"],"metadata":{"id":"GoFReL0bEj1I"}},{"cell_type":"code","source":["dates = ['2018-01', '2018-02']\n","# dates = ['2018-01']\n","\n","raw_train_validation_group = df_raw_train.groupby(df_raw_train['time'].str.contains('|'.join(dates)))\n","df_raw_train = raw_train_validation_group.get_group(False).copy()\n","df_raw_validation = raw_train_validation_group.get_group(True).copy()\n"],"metadata":{"id":"bcSHW_v-foR1","executionInfo":{"status":"ok","timestamp":1696721968500,"user_tz":180,"elapsed":18,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["Apos essa separação, possuimos 3 bases:\n","- df_raw_train: Base para treinamento\n","- df_raw_validation: Base para validação\n","- df_raw_test: Base para teste"],"metadata":{"id":"aoymA1rlWMcC"}},{"cell_type":"markdown","source":["Remoção das colunas elevation e sweep pois não possuiam valor agregado na base\n","\n","Para verificar que as colunas elevation e sweep não possuiam valor agregado, foi utilizado a função describe do pandas. Com esta função, foi possível verificar que ambas possuiam média, minimo e maximo identicos, além de um desvio padrão igual a 0, ou seja, todas as linhas possuiam o mesmo valor."],"metadata":{"id":"x2L_kDfHfqD-"}},{"cell_type":"code","source":["print(df_raw_train.describe()['elevation'], end=\"\\n\\n\")\n","print(df_raw_train.describe()['sweep'])\n","\n","df_raw_train.drop(['elevation', 'sweep'], axis=1, inplace=True)"],"metadata":{"id":"8pRJLTkafp4_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696721969031,"user_tz":180,"elapsed":547,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}},"outputId":"93359a17-83d5-4128-8772-1ebc96a1b4da"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["count    83083.0\n","mean         0.5\n","std          0.0\n","min          0.5\n","25%          0.5\n","50%          0.5\n","75%          0.5\n","max          0.5\n","Name: elevation, dtype: float64\n","\n","count    83083.0\n","mean         0.0\n","std          0.0\n","min          0.0\n","25%          0.0\n","50%          0.0\n","75%          0.0\n","max          0.0\n","Name: sweep, dtype: float64\n"]}]},{"cell_type":"markdown","source":["### Etapa de clusterização dos dados por estação\n","\n","Criando 2 DataFrames onde um deles armazena as informações das colunas X e Y da base e o outro armazena as colunas LAT e LON\n"],"metadata":{"id":"5HvbucbofsGI"}},{"cell_type":"code","source":["grouped_by_est = df_raw_train.groupby(['Est'])\n","\n","df_xy = pd.DataFrame(columns=['est','x', 'y'])\n","df_lat_lon = pd.DataFrame(columns=['est', 'lat', 'lon'])\n","for est in grouped_by_est.groups.keys():\n","    group = grouped_by_est.get_group(est)\n","\n","    list_xy = [est, group['x'].mean(), group['y'].mean()]\n","    df_xy = pd.concat([pd.DataFrame([list_xy], columns=df_xy.columns), df_xy], ignore_index=True)\n","\n","    list_lat_lon = [est, group['lat'].mean(), group['lon'].mean()]\n","    df_lat_lon = pd.concat([pd.DataFrame([list_lat_lon], columns=df_lat_lon.columns), df_lat_lon], ignore_index=True)"],"metadata":{"id":"I2fIz1eQfunb","executionInfo":{"status":"ok","timestamp":1696719130450,"user_tz":180,"elapsed":391,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Verificando que os valores X e Y fazer o mesmo papel que os valores de LAT e LON para o calcula da distância entre as estações"],"metadata":{"id":"gd0XK__jf_56"}},{"cell_type":"code","source":["def rad2deg(radians):\n","    degrees = radians * 180 / pi\n","    return degrees\n","\n","def deg2rad(degrees):\n","    radians = degrees * pi / 180\n","    return radians\n","\n","theta = df_lat_lon['lon'][0] - df_lat_lon['lon'][1]\n","distance = 60 * 1.1515 * rad2deg(\n","    arccos(\n","        (sin(deg2rad(df_lat_lon['lat'][0])) * sin(deg2rad(df_lat_lon['lat'][1]))) +\n","        (cos(deg2rad(df_lat_lon['lat'][0])) * cos(deg2rad(df_lat_lon['lat'][1])) * cos(deg2rad(theta)))\n","    )\n",")\n","distance = round(distance * 1.609344, 5)\n","print(distance)\n","\n","# Distância euclidina por meio do numpyu\n","point1 = np.array((df_xy['x'][0], df_xy['y'][0]))\n","point2 = np.array((df_xy['x'][1], df_xy['y'][1]))\n","distance = np.linalg.norm(point1 - point2)\n","print(round(distance / 1000, 5))\n"],"metadata":{"id":"zBLQy-5jgCGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotando um gráfico das posições das estações para ter um auxilio visual na clusterização\n"],"metadata":{"id":"8vgdpy0BgE9X"}},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","for data in df_lat_lon.iterrows():\n","    ax.scatter(data[1][1], data[1][2], label=f'{data[1][0]} - {data[0]}')\n","\n","colormap = plt.cm.gist_ncar\n","colorst = [colormap(i) for i in np.linspace(0.1, 0.9,len(ax.collections))]\n","for t, j1 in enumerate(ax.collections):\n","    j1.set_color(colorst[t])\n","\n","for data in df_lat_lon.iterrows():\n","    ax.annotate(data[0], (data[1][1], data[1][2]))\n","\n","ax.legend(bbox_to_anchor=(1, 1), bbox_transform=ax.transAxes, fontsize='small')\n","plt.show()\n","\n","# fig = plt.figure()\n","# ax = fig.add_subplot(111)\n","# for data in df_xy.iterrows():\n","#     ax.scatter(data[1][1], data[1][2], label=f'{data[1][0]} - {data[0]}')\n","\n","# colormap = plt.cm.gist_ncar\n","# colorst = [colormap(i) for i in np.linspace(0.1, 0.9,len(ax.collections))]\n","# for t, j1 in enumerate(ax.collections):\n","#     j1.set_color(colorst[t])\n","\n","# for data in df_xy.iterrows():\n","#     ax.annotate(data[0], (data[1][1], data[1][2]))\n","\n","# ax.legend(bbox_to_anchor=(1, 1), bbox_transform=ax.transAxes, fontsize='small')\n","# plt.show()"],"metadata":{"id":"Kl4-8nMkgGS6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculando a distancia de cada estação entre si para ter um embasamento numérico na decisão da clusterização"],"metadata":{"id":"mvnctHtRgIU2"}},{"cell_type":"code","source":["def euclidian_distance(x1, y1, x2, y2):\n","    p1 = np.array((x1, y1))\n","    p2 = np.array((x2, y2))\n","    return np.linalg.norm(p1 - p2)\n","\n","est_distances = []\n","for est in df_xy.iterrows():\n","    distances = []\n","    for row in df_xy.iterrows():\n","        if est[1][0] != row[1][0]:\n","            distance = round((euclidian_distance(est[1][1], est[1][2], row[1][1], row[1][2]) / 1000), 3)\n","            distances.append((distance, row[1][0]))\n","    distances = sorted(distances)\n","    distances_sum = sum(i[0] for i in distances[:6])\n","    est_distances.append((distances_sum, est[1][0], distances))\n","\n","est_distances = sorted(est_distances)\n","\n","for est in est_distances:\n","    print(f'Estação: {est[1]} - Soma: {est[0]} - Estações: {est[2][:6]}')"],"metadata":{"id":"EeXE_ffYgJqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remoção das colunas X, Y, Z, LAT, LON, ALT, ja que a partir de agora, a clusterização vai trazer a informação de posicionamento"],"metadata":{"id":"dQq6cx7lgLvM"}},{"cell_type":"code","source":["df_raw_train.drop(['x', 'y', 'z', 'lat', 'lon', 'alt'], axis=1, inplace=True)"],"metadata":{"id":"xOtQYwKXgNFi","executionInfo":{"status":"ok","timestamp":1696719415030,"user_tz":180,"elapsed":12,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Após alguns testes e tentativas de divisão, a clusterização final ficou como:\n"],"metadata":{"id":"PjLnoEbxgOk4"}},{"cell_type":"code","source":["grouped_by_est = df_raw_train.groupby(['Est'])\n","dict_df_train = {'df_train' : {}, 'for_input': {}}\n","\n","groups = [\n","    ['Pato_Branco', 'Laranjeiras_do_Sul', 'Segredo', 'Derivacao_do_Rio_Jordao', 'Coronel_Domingos_Soares', 'Solais_Novo'],\n","    ['Cascavel', 'Baixo_Iguacu', 'Salto_Caxias', 'Reservatorio_Salto_Caxias', 'Boa_Vista_da_Aparecida', 'Porto_Santo_Antonio', 'Aguas_do_Vere', 'Bela_Vista_Jusante'],\n","    ['Foz_do_Iguacu_-_Itaipu', 'Santa_Helena', 'Guaira', 'Palotina', 'Toledo', 'Assis_Chateaubriand', 'Altonia'],\n","    ['Loanda', 'Paranavai', 'Campo_Mourao', 'Umuarama', 'Ubirata', 'Porto_Formosa']\n","]\n","\n","for group in groups:\n","    df_groupped = grouped_by_est.get_group(group[0])\n","    for index in range(1, len(group)):\n","        df_groupped = pd.concat([grouped_by_est.get_group(group[index]), df_groupped], ignore_index=True)\n","    df_groupped.drop(['Est'], axis=1, inplace=True)\n","\n","    label = ' - '.join(map(str, group))\n","    dict_df_train['df_train'][label] = df_groupped\n","\n","\n","for group in dict_df_train['df_train']:\n","    df_groupped = dict_df_train[\"df_train\"][group].dropna()\n","    columns = list(df_groupped.head())\n","\n","    dict_df_train['for_input'][group] = {}\n","    for row in df_groupped.iterrows():\n","        timestamp = datetime.strptime(row[1][\"time\"], '%Y-%m-%d %H:%M:%S').timestamp()\n","\n","        row_info = {}\n","        for col in columns:\n","            row_info[col] = row[1][col]\n","\n","        dict_df_train['for_input'][group][timestamp] = row_info\n"],"metadata":{"id":"eyS8eQrqgPWw","executionInfo":{"status":"ok","timestamp":1696721987109,"user_tz":180,"elapsed":5901,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### Tratamento dos dados faltantes\n","\n","O dicionário 'for_input' produzido acima possui as seguintes características:\n","- Dividido em grupos da mesma forma que os datasets de treinamento;\n","- Possui apenas as linhas que possuiam todos os dados dos sensores dos datasets de treinamento;\n","- Foi organizado por meio de dicionários internos com a chave sendo o timestamp do dado para facilitar a busca.\n","\n","Com este dicionário, o tratamento dos dados faltantes foi realizado de forma clusterizada, ou seja, on inputs foram realizados apenas com dados do própio grupo"],"metadata":{"id":"peBJO6qMgSR4"}},{"cell_type":"code","source":["def get_data(row, input_values):\n","    timestamp = datetime.strptime(row[\"time\"], '%Y-%m-%d %H:%M:%S').timestamp()\n","\n","    data = None\n","    min_distance = math.inf\n","    for i in input_values:\n","        distance = abs(timestamp - i)\n","        if (distance < min_distance):\n","            min_distance = distance\n","            data = input_values[i]\n","\n","    return data\n","\n","def df_input_data(df_group, input_values):\n","    columns = list(df_group.head())\n","    for row in df_group.iterrows():\n","        replace_data = get_data(row[1], input_values)\n","        for col in columns:\n","            if str(row[1][col]) == 'nan':\n","                df_group.loc[row[0], col] = replace_data[col]\n","\n","for group in dict_df_train['df_train']:\n","    df_group = dict_df_train['df_train'][group]\n","    input_values = dict_df_train['for_input'][group]\n","    df_input_data(df_group, input_values)\n","    dict_df_train['df_train'][group].drop(['time'], axis=1, inplace=True)"],"metadata":{"id":"W4Mzex0VgS4v","executionInfo":{"status":"ok","timestamp":1696722104670,"user_tz":180,"elapsed":104405,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["### Preparação da base de validação\n","\n","Removendo as colunas da base de validação que foram removidas da base de treinamento"],"metadata":{"id":"KC3RNCn1gVI5"}},{"cell_type":"code","source":["# df_validation = df_raw_validation.drop(['elevation', 'sweep', 'x', 'y', 'z', 'lat', 'lon', 'alt'], axis=1)\n","df_validation = df_raw_validation.drop(['elevation', 'sweep'], axis=1)"],"metadata":{"id":"QwSpTRbqgWa8","executionInfo":{"status":"ok","timestamp":1696722156921,"user_tz":180,"elapsed":383,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Realizando o tratamento de dados faltantes na base de validação. O input foi feito da seguinte maneira:\n","\n","Para realizar o tratamento, primeiro, para cada linha, é buscado o grupo a qual ela pertence, ou seja, de qual grupo de estações o dado veio. Após isso, apenas a base de treinamento é utilizada para o tratamento, sem a influência dos dados da base de validação."],"metadata":{"id":"v4qoAnOTgYiL"}},{"cell_type":"code","source":["def get_group(est, input_groups):\n","    for group in input_groups:\n","        if est in group: return input_groups[group]\n","\n","columns = list(df_validation.head())\n","for row in df_validation.iterrows():\n","    input_values = get_group(row[1]['Est'], dict_df_train['for_input'])\n","    replace_data = get_data(row[1], input_values)\n","    for col in columns:\n","        if str(row[1][col]) == 'nan':\n","            df_validation.loc[row[0], col] = replace_data[col]\n","\n","df_validation.drop(['Est', 'time'], axis=1, inplace=True)"],"metadata":{"id":"JvqWXQzjgX1n","executionInfo":{"status":"ok","timestamp":1696722165853,"user_tz":180,"elapsed":7380,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["### Validação dos modelos de regressão\n","\n","To-do: Utilizar a base de validação para definir o modelo que vou usar"],"metadata":{"id":"M24ZdsFfgbhQ"}},{"cell_type":"code","source":["def get_model(model):\n","    match model:\n","        case 'knn':\n","            return neighbors.KNeighborsRegressor(n_neighbors=100)\n","        case 'svr':\n","            return svm.SVR()\n","        case 'linear_regression':\n","            return linear_model.LinearRegression()\n","        case 'tree_regression':\n","            return tree.DecisionTreeRegressor()\n","\n","models = ['knn', 'svr', 'linear_regression', 'tree_regression']\n","\n","validate_list = np.array(df_validation.values.tolist())\n","x_validate = validate_list[:, :-1]\n","y_validate = validate_list[:, -1]\n","\n","preds = {'knn': [], 'svr': [], 'linear_regression': [], 'tree_regression': []}\n","for group in dict_df_train['df_train']:\n","    train_list = np.array(dict_df_train['df_train'][group].values.tolist())\n","    x_train = train_list[:, :-1]\n","    y_train = train_list[:, -1]\n","\n","    for model_type in models:\n","        model = get_model(model_type)\n","        model.fit(x_train, y_train)\n","\n","        pred = model.predict(x_validate)\n","        preds[model_type].append(pred)\n","\n","avarages = {'knn': [], 'svr': [], 'linear_regression': [], 'tree_regression': []}\n","for model_pred in preds:\n","    avarage = preds[model_pred][0]\n","    for index in range(1, len(preds[model_pred])):\n","        avarage = [a + b for a, b in zip(avarage, preds[model_pred][index])]\n","    avarage = [x / len(preds[model_pred]) for x in avarage]\n","\n","    avarages[model_pred] = avarage\n","\n","results = {'mse': [], 'mae': []}\n","for avarage in avarages:\n","    mse = mean_squared_error(y_validate, avarages[avarage])\n","    mae = mean_absolute_error(y_validate, avarages[avarage])\n","    results['mse'].append(mse)\n","    results['mae'].append(mae)\n","\n","    print(f'Modelo: {avarage} - MSE: {mse} - MAE: {mae}')"],"metadata":{"id":"x-77quHJu4An","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696722347397,"user_tz":180,"elapsed":147683,"user":{"displayName":"Paulo Mateus Luza Alves","userId":"12822114180034320779"}},"outputId":"a0a9d76f-2898-4de2-8097-468e40f8e89b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo: knn - MSE: 6.043251417520959 - MAE: 1.257230230966638\n","Modelo: svr - MSE: 6.721039461352233 - MAE: 1.014354361004017\n","Modelo: linear_regression - MSE: 259.262143943076 - MAE: 11.659095839978125\n","Modelo: tree_regression - MSE: 8.958345470487597 - MAE: 1.9542386655260908\n"]}]},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","for index, model in enumerate(models):\n","    ax.scatter(results['mae'][index], results['mse'][index], label=f'mse: {results[\"mse\"][index]} - mae: {results[\"mae\"][index]}')\n","\n","for index, model in enumerate(models):\n","    ax.annotate(model, (results['mae'][index], results['mse'][index]))\n","\n","ax.legend(bbox_to_anchor=(1.2, 1), bbox_transform=ax.transAxes, fontsize='small')\n","plt.show()"],"metadata":{"id":"27j9zAuyM8vC"},"execution_count":null,"outputs":[]}]}