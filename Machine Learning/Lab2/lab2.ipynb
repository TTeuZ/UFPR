{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import sin, cos, arccos, pi, round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo a CSV e removendo todos os dados com valores de Tp_est iguais a zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data = pd.read_csv('data.csv')\n",
    "df_raw_data.drop(df_raw_data[df_raw_data['Tp_est'] == 0.0].index, inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção inicial de colunas da base geral (2018 a 2022). A colunas removidas foram:\n",
    "- Unnamed: 0 pois é a coluna de ids\n",
    "- latitude e longitude pois possuem a mesma informação que as colunas lat e lon;\n",
    "- distancia: Não agrega valor ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data.drop(['Unnamed: 0', 'latitude', 'longitude', 'distancia'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando o raw data entre treinamento e teste. Anos de 2018 a 2021 para treinamento e 2022 para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_test_group = df_raw_data.groupby(df_raw_data['time'].str.contains('2022'))\n",
    "\n",
    "df_raw_train = raw_train_test_group.get_group(False).copy()\n",
    "df_raw_test = raw_train_test_group.get_group(True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção das colunas elevation e sweep pois não possuiam valor agregado na base\n",
    "\n",
    "Para verificar que as colunas elevation e sweep não possuiam valor agregado, foi utilizado a função describe do pandas. Com esta função, foi possível verificar que ambas possuiam média, minimo e maximo identicos, além de um desvio padrão igual a 0, ou seja, todas as linhas possuiam o mesmo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_raw_train.describe()['elevation'], end=\"\\n\\n\")\n",
    "print(df_raw_train.describe()['sweep'])\n",
    "\n",
    "df_raw_train.drop(['elevation','sweep'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando 2 DataFrames onde um deles armazena as informações das colunas X e Y da base e o outro armazena as colunas LAT e LON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_est = df_raw_train.groupby(['Est'])\n",
    "\n",
    "df_xy = pd.DataFrame(columns=['est','x', 'y'])\n",
    "df_lat_lon = pd.DataFrame(columns=['est', 'lat', 'lon'])\n",
    "for est in grouped_by_est.groups.keys():\n",
    "    group = grouped_by_est.get_group(est)\n",
    "\n",
    "    list_xy = [est, group['x'].mean(), group['y'].mean()]\n",
    "    df_xy = pd.concat([pd.DataFrame([list_xy], columns=df_xy.columns), df_xy], ignore_index=True)\n",
    "\n",
    "    list_lat_lon = [est, group['lat'].mean(), group['lon'].mean()]\n",
    "    df_lat_lon = pd.concat([pd.DataFrame([list_lat_lon], columns=df_lat_lon.columns), df_lat_lon], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando que os valores X e Y fazer o mesmo papel que os valores de LAT e LON para o calcula da distância entre as estações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rad2deg(radians):\n",
    "    degrees = radians * 180 / pi\n",
    "    return degrees\n",
    "\n",
    "def deg2rad(degrees):\n",
    "    radians = degrees * pi / 180\n",
    "    return radians\n",
    "\n",
    "theta = df_lat_lon['lon'][0] - df_lat_lon['lon'][1]\n",
    "distance = 60 * 1.1515 * rad2deg(\n",
    "    arccos(\n",
    "        (sin(deg2rad(df_lat_lon['lat'][0])) * sin(deg2rad(df_lat_lon['lat'][1]))) + \n",
    "        (cos(deg2rad(df_lat_lon['lat'][0])) * cos(deg2rad(df_lat_lon['lat'][1])) * cos(deg2rad(theta)))\n",
    "    )\n",
    ")\n",
    "distance = round(distance * 1.609344, 5)\n",
    "print(distance)\n",
    "\n",
    "# Distância euclidina por meio do numpyu\n",
    "point1 = np.array((df_xy['x'][0], df_xy['y'][0]))\n",
    "point2 = np.array((df_xy['x'][1], df_xy['y'][1]))\n",
    "distance = np.linalg.norm(point1 - point2)\n",
    "print(round(distance / 1000, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando um gráfico das posições das estações para ter um auxilio visual na clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for data in df_xy.iterrows():\n",
    "    ax.scatter(data[1][1], data[1][2], label=f'{data[1][0]} - {data[0]}')\n",
    "\n",
    "colormap = plt.cm.gist_ncar\n",
    "colorst = [colormap(i) for i in np.linspace(0.1, 0.9,len(ax.collections))]\n",
    "for t, j1 in enumerate(ax.collections):\n",
    "    j1.set_color(colorst[t])\n",
    "\n",
    "for data in df_xy.iterrows():\n",
    "    ax.annotate(data[0], (data[1][1], data[1][2]))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1, 1), bbox_transform=ax.transAxes, fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando a distancia de cada estação entre si para ter um embasamento numérico na decisão da clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(x1, y1, x2, y2):\n",
    "    p1 = np.array((x1, y1))\n",
    "    p2 = np.array((x2, y2))\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "est_distances = []\n",
    "for est in df_xy.iterrows():\n",
    "    distances = []\n",
    "    for row in df_xy.iterrows():\n",
    "        if est[1][0] != row[1][0]:\n",
    "            distance = round((euclidian_distance(est[1][1], est[1][2], row[1][1], row[1][2]) / 1000), 3)\n",
    "            distances.append((distance, row[1][0]))\n",
    "    distances = sorted(distances)\n",
    "    distances_sum = sum(i[0] for i in distances[:6])\n",
    "    est_distances.append((distances_sum, est[1][0], distances))\n",
    "\n",
    "est_distances = sorted(est_distances)\n",
    "\n",
    "for est in est_distances:\n",
    "    print(f'Estação: {est[1]} - Soma: {est[0]} - Estações: {est[2][:6]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção das colunas X, Y, Z, LAT, LON, ALT, ja que a partir de agora, a clusterização vai trazer a informação de posicionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_train.drop(['x', 'y', 'z', 'lat', 'lon', 'alt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após alguns testes e tentativas de divisão, a clusterização final ficou como:\n",
    "- Grupo 1: Laranjeiras_do_Sul, Segredo, Derivacao_do_Rio_Jordao, Coronel_Domingos_Soares, Solais_Novo\n",
    "- Grupo 2: Pato_Branco, Bela_Vista_Jusante, Aguas_do_Vere\n",
    "- Grupo 3: Baixo_Iguacu, Salto_Caxias, Reservatorio_Salto_Caxias, Boa_Vista_da_Aparecida, Porto_Santo_Antonio\n",
    "- Grupo 4: Santa_Helena, Foz_do_Iguacu_-_Itaipu\n",
    "- Grupo 5: Toledo, Cascavel, Assis_Chateaubriand\n",
    "- Grupo 6: Guaira, Palotina, Altonia\n",
    "- Grupo 7: Umuarama, Ubirata, Porto_Formosa\n",
    "- Grupo 8: Loanda, Paranavai, Campo_Mourao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_est = df_raw_train.groupby(['Est'])\n",
    "dict_df_train = {'df_train' : {}, 'for_input': {}}\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Laranjeiras_do_Sul')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Segredo'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Derivacao_do_Rio_Jordao'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Coronel_Domingos_Soares'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Solais_Novo'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Laranjeiras_do_Sul - Segredo - Derivacao_do_Rio_Jordao - Coronel_Domingos_Soares - Solais_Novo'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Pato_Branco')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Bela_Vista_Jusante'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Aguas_do_Vere'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Pato_Branco - Bela_Vista_Jusante - Aguas_do_Vere'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Baixo_Iguacu')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Salto_Caxias'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Reservatorio_Salto_Caxias'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Boa_Vista_da_Aparecida'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Porto_Santo_Antonio'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Baixo_Iguacu - Salto_Caxias - Reservatorio_Salto_Caxias - Boa_Vista_da_Aparecida - Porto_Santo_Antonio'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Santa_Helena')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Foz_do_Iguacu_-_Itaipu'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Santa_Helena - Foz_do_Iguacu_-_Itaipu'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Toledo')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Cascavel'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Assis_Chateaubriand'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Toledo - Cascavel - Assis_Chateaubriand'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Guaira')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Palotina'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Altonia'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Guaira - Palotina - Altonia'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Umuarama')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Ubirata'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Porto_Formosa'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Umuarama - Ubirata - Porto_Formosa'] = df_groupped\n",
    "\n",
    "df_groupped = grouped_by_est.get_group('Loanda')\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Paranavai'), df_groupped], ignore_index=True)\n",
    "df_groupped = pd.concat([grouped_by_est.get_group('Campo_Mourao'), df_groupped], ignore_index=True)\n",
    "dict_df_train['df_train']['Loanda - Paranavai - Campo_Mourao'] = df_groupped\n",
    "\n",
    "for group in dict_df_train['df_train']:\n",
    "    df_groupped = dict_df_train[\"df_train\"][group].dropna()\n",
    "    columns = list(df_groupped.head())\n",
    "\n",
    "    dict_df_train['for_input'][group] = {}\n",
    "    for row in df_groupped.iterrows():\n",
    "        timestamp = datetime.strptime(row[1][\"time\"], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "        row_info = {}\n",
    "        for col in columns:\n",
    "            row_info[col] = row[1][col]\n",
    "\n",
    "        dict_df_train['for_input'][group][timestamp] = row_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dicionário 'for_input' produzido acima possui as seguintes características:\n",
    "- Dividido em grupos da mesma forma que os datasets de treinamento;\n",
    "- Possui apenas as linhas que possuiam todos os dados dos sensores dos datasets de treinamento;\n",
    "- Foi organizado por meio de dicionários internos com a chave sendo o timestamp do dado para facilitar a busca.\n",
    "\n",
    "Com este dicionário, o data input foi realizado de forma clusterizada, ou seja, on inputs foram realizados apenas com dados do própio grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(row, col, input_values):\n",
    "    timestamp = datetime.strptime(row[\"time\"], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "    value = None\n",
    "    min_distance = math.inf\n",
    "    for i in input_values:\n",
    "        distance = abs(timestamp - i)\n",
    "        if (distance < min_distance):\n",
    "            min_distance = distance\n",
    "            value = input_values[i][col]\n",
    "            \n",
    "    return value\n",
    "\n",
    "def df_input_data(df_group, input_values):\n",
    "    columns = list(df_group.head())\n",
    "    for row in df_group.iterrows():\n",
    "        for col in columns:\n",
    "            if str(row[1][col]) == 'nan':\n",
    "                df_group.loc[row[0], col] = get_data(row[1], col, input_values)\n",
    "\n",
    "for group in dict_df_train['df_train']:\n",
    "    df_group = dict_df_train['df_train'][group]\n",
    "    input_values = dict_df_train['for_input'][group]\n",
    "    df_input_data(df_group, input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Fazer o data inputation da base de teste seguindo os valores da base de treinamento, e remover as colunas da base de teste que foram removidas na de treinamento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
